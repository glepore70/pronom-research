
Template-Type: ReDIF-Paper 1.0
Author-Name:  Phil Ender
Author-Workplace-Name: UCLA 
Author-Email:  ender@ucla.edu
Title: The Multivariate Dustbin
Abstract: When I was in graduate school I was taught that multivariate methods were the future of data analysis. In that dark computer stone age multivariate meant multivariate analysis of variance (MANOVA), linear discriminant function analysis (LDA), canonical correlation analysis (CA) and factor analysis (which will not be discussed in this presentation). Â Statistical software has evolved considerably since those ancient days. Â MANOVA, LDA and CA are still around but have been eclipsed and pushed aside by newer, sexier methodologies. These three methods have been consigned to the multivariate dustbin, so to speak. Â This presentation will review MANOVA, LDA and CA, discuss the connections among the three approaches and highlight the positives and negatives or each approach. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Ender.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:1

Template-Type: ReDIF-Paper 1.0
Author-Name:  Billy Buchanan
Author-Workplace-Name: Fayette County Public Schools
Author-Email:  billy.buchanan@fayette.kyschools.us
Title: Stata Extensibility with the Java API: Tools, Examples, and Advice
Abstract: The inclusion of the Java API for Stata provides users, and user programmers, with exciting opportunities to leverage a wide array of existing work in the context of their Stata workflow.  This talk will introduce a few tools designed to help others wanting to integrate Java libraries into their workflow – the Stata Maven Archetype and the StataJavaUtilities library.  In addition to a higher level overview, the presentation will also show others examples of using existing Java libraries to expand statistical models in psychometrics, send yourself emails when your job is complete, phonetic string encodings and string distances, accessing file/operating system properties, and others that they can use as starting points for developing their own Java plugins in Stata.   
File-URL: https://wbuchanan.github.io/stataConference2017/#/
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:2

Template-Type: ReDIF-Paper 1.0
Author-Name:  Brian Quistorff
Author-Workplace-Name: Microsoft
Author-Email:  brian@quistorff.com
Author-Name: George G Vega Yon
Author-Workplace-Name: University of Southern California
Title: Uncomplicated Parallel Computing with Stata
Abstract: Parallel lets you run Stata faster, sometimes faster than MP itself. By organizing your job in several Stata instances, parallel allows you to work with out-of-the-box parallel computing. Using the the 'parallel' prefix, you can get faster simulations, bootstrapping, reshaping big data, etc. without having to know a thing about parallel computing. With no need of having Stata/MP installed on your computer, parallel has showed to dramatically speedup computations up to two, four, or more times depending on how many processors your computer has. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Quistorff.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:3

Template-Type: ReDIF-Paper 1.0
Author-Name:  Nicholas Cox
Author-Workplace-Name: Durham University, UK
Author-Email:  njcoxstata@gmail.com
Author-Person: pco34
Title: On the shoulders of giants, or not reinventing the wheel
Abstract: Part of the art of coding is writing as little as possible to do as much as possible. The presentation expands on this truism. Examples are given of Stata code to yield graphs and tables in which most of the real work is happily delegated to workhorse commands. 
In graphics a key principle is that -graph twoway- is the most general command, even when you do not want rectangular axes. Variations on scatter and line plots are precisely that, variations on scatter and line plots. More challenging illustrations include commands for circular and triangular graphics, in which x and y axes are omitted, with an inevitable but manageable cost in re-creating scaffolding, titles, labels and other elements. 
In tabulations and listings the better known commands sometimes seem to fall short of what you want. However, a few preparation commands (such as -generate-, -egen-,    -collapse- or -contract-) followed by -list-, -tabdisp- or -_tab- can get you a long way. 
The examples range in scope from a few lines of interactive code to fully developed programs. The presentation is thus pitched at all levels of Stata users. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Cox.pptx
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:4

Template-Type: ReDIF-Paper 1.0
Author-Name:  Dario Sansone
Author-Workplace-Name: Georgetown University
Author-Email:  ds1289@georgetown.edu
Author-Person: psa1336
Title: Now You See Me: High School Dropout and Machine Learning
Abstract: In this paper, we create an algorithm to predict which students are eventually going to drop out of US high school using information available in 9th grade. We show that using a naive model - as implemented in many schools - leads to poor predictions. In addition to this, we explain how schools can obtain more precise predictions by exploiting the big data available to them, as well as more sophisticated quantitative techniques. We also compare the performances of econometric techniques like Logistic Regression with Machine Learning tools such as Support Vector Machine, Boosting and LASSO. We offer practical advice on how to apply the new Machine Learning codes available in Stata to the high dimensional datasets available in education. Model parameters are calibrated by taking into account policy goals and budget constraints.  
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Sansone.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:5

Template-Type: ReDIF-Paper 1.0
Author-Name:  Sergio Correia
Author-Workplace-Name: Board of Governors of the Federal Reserve System
Author-Email:  sergio.correia@gmail.com
Author-Person: pco826
Title: Big data in Stata with the ftools package
Abstract: In recent years, very large datasets have become increasingly prevalent in most social sciences. However, some of the most important Stata commands (collapse, egen, merge, sort, etc.) rely on algorithms that are not well suited for big data.
In my talk, I will present the ftools package, which contains plug-in alternatives to these commands and performs up to 20 times faster on large datasets.
Further, I will explain the underlying algorithm and Mata function, and show how to use this function to create new Stata commands and to speed up existing packages.
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Correia.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:6

Template-Type: ReDIF-Paper 1.0
Author-Name:  Christopher Baum
Author-Workplace-Name: Boston College 
Author-Workplace-Name: DIW Berlin
Author-Email:  baum@bc.edu
Author-Person: pba1
Author-Name: Jesús Otero
Author-Workplace-Name: Universidad del Rosario, Colombia
Author-Person: pot11
Title: Response surface models for the Elliott, Rothenberg, Stock DF-GLS unit root test
Abstract: We present response surface coefficients for a large range of quantiles of the Elliott, Rothenberg and Stock (Econometrica, 1996) DF-GLS unit root tests, for different combinations of the number of observations and the lag order in the test regressions, where the latter can be either specified by the user or endogenously determined. The critical values depend on the method used to select the number of lags. The Stata command ersur is presented, and its use illustrated with an empirical example that tests the validity of the expectations hypothesis of the term structure of interest rates. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Baum.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:7

Template-Type: ReDIF-Paper 1.0
Author-Name:  Shawn Bauldry
Author-Workplace-Name: Purdue University
Author-Email:  sbauldry@purdue.edu
Author-Name: Jun Xu
Author-Workplace-Name: Ball State University
Author-Person: pxu111
Author-Name: Andrew Fullerton
Author-Workplace-Name: Oklahoma State University
Title: crreg: A New Command for Generalized Continuation Ratio Models
Abstract: A continuation ratio model represents a variant of an ordered regression model that is suited to modeling processes that unfold in stages, such as educational attainment. The parameters for covariates in continuation ratio models may be constrained to be equal, subject to a proportionality constraint across stages, or freely vary across stages. Currently there are three user-written Stata commands that fit continuation ratio models. Each of these commands fit some subset of continuation ratio models involving parameter constraints, but none of them offer complete coverage of the range of possibilities. In addition, all of the commands rely on reshaping the data into a stage-case format to facilitate estimation. The new crreg command expands the options for continuation ratio models to include the possibility for some or all of the covariates to be constrained to be equal, freely vary, or have a proportionality constraint across stages. The crreg command relies on Stata’s ML routines for estimation and avoids reshaping the data. The crreg command includes options for three different link functions (the logit, probit, and the cloglog) and supports Stata’s survey and multiple imputation suites of commands. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Bauldry.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:8

Template-Type: ReDIF-Paper 1.0
Author-Name: Hua Peng
Author-Workplace-Name: StataCorp
Title: Incorporating Stata into reproducible documents
Abstract: Part of reproducible research is eliminating manual steps such as hand-editing documents. Stata 15 introduces several commands which facilitate automated document production, including dyndoc for converting dynamic Markdown documents to web pages, putdocx for creating Word documents, and putpdf for creating PDF files. These commands allow you to mix formatted text and Stata output, and allow you to embed Stata graphs, in-line Stata results, and tables containing the output from selected Stata commands. We will show these commands in action, demonstrating automating the production of documents in various formats, and including Stata results in those documents.
File-URL: https://huapeng01016.github.io/baltimore_2017/birds17.html
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:9

Template-Type: ReDIF-Paper 1.0
Author-Name: Xiao Yang
Author-Workplace-Name: StataCorp
Title: Analyzing interval-censored survival-time data in Stata
Abstract: In survival analysis, right-censored data have been studied extensively and can be analyzed using Stata's extensive suite of survival commands, including streg for fitting parametric survival models. Right-censored data are a special case of interval-censored data. Interval-censoring occurs when the failure time of interest is not exactly observed but is only known to lie within some interval. Left-censoring, which occurs when the failure is known to happen some time before the observed time, is also a special case of interval-censoring. Survival data may contain a mixture of uncensored, right-censored, left-censored, and interval-censored observations. In this talk, I will describe basic types of interval-censored data and demonstrate how to fit parametric survival models to these data using Stata's new stintreg command. I will also discuss postestimation features available after this command.
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Yang.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:10

Template-Type: ReDIF-Paper 1.0
Author-Name:  John Gallis
Author-Workplace-Name: Department of Biostatistics and Bioinformatics, and Duke Global Health Institute, Duke University
Author-Email:  john.gallis@duke.edu
Author-Name: Fan Li 
Author-Workplace-Name: Department of Biostatistics and Bioinformatics, Duke University
Author-Name:  Hengshi Yu 
Author-Workplace-Name: Department of Biostatistics and Bioinformatics, Duke University
Author-Name: Elizabeth L. Turner 
Author-Workplace-Name: Department of Biostatistics and Bioinformatics, Duke University
Title: cvcrand and cptest: Efficient design and analysis of cluster randomized trials
Abstract: Cluster randomized trials (CRTs), where clusters (for example, schools or clinics) are randomized but measurements are taken on individuals, are commonly used to evaluate interventions in public health and social science. Because CRTs typically involve only a few clusters, simple randomization frequently leads to baseline imbalance of cluster characteristics across treatment arms, threatening the internal validity of the trial. In CRTs with a small number of clusters, classic approaches to balancing baseline characteristics—such as matching and stratification—have several drawbacks, especially when the number of baseline characteristics the researcher desires to balance is large (Ivers et al. 2012). An alternative approach is constrained randomization, whereby an allocation scheme is randomly selected from a subset of all possible allocation schemes based on the value of a balancing criterion (Raab and Butcher 2001). Subsequently, an adjusted permutation test can be used in the analysis, which provides increased efficiency under constrained randomization compared with simple randomization (Li et al. 2015). We describe constrained randomization and permutation tests for the design and analysis of CRTs and provide examples to demonstrate the use of our newly created Stata package (cvcrand), which uses Mata to efficiently process large allocation matrices—to implement constrained randomization and permutation tests.
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Gallis.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:11

Template-Type: ReDIF-Paper 1.0
Author-Name:  Daniel C. Schneider
Author-Workplace-Name: Max Planck Institute for Demographic Research
Author-Email:  schneider@demogr.mpg.de
Title: Working with Demographic Life Table Data in Stata
Abstract: This presentation introduces two user-written Stata commands related to the data and calculations of demographic life tables, whose most prominent feature is the calculation of life expectancy at birth. The first command, -hmddata-, provides a convenient interface to the Human Mortality Database (HMD, www.mortality.org), a database widely used for mortality data by demographers, health researchers, and social scientists. Different subcommands of -hmddata- allow data from this database to be easily loaded, transformed, reshaped, tabulated, and graphed. The second command, -lifetable-, produces demographic period life tables. The main features are: Life table columns can be flexibly calculated using any valid minimum starting information; abridged tables can be generated from complete ones; finally, a Stata data set can hold any number of life tables, and the various -lifetable- subcommands can operate on any subset of them. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Schneider.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:12

Template-Type: ReDIF-Paper 1.0
Author-Name:  Austin Nichols
Author-Workplace-Name: Abt Associates
Author-Email:  austinnichols@gmail.com
Author-Person: pni54
Author-Name: Linden McBride
Author-Workplace-Name: Cornell University
Author-Person: pmc244
Title: Propensity Scores and Causal Inference Using Machine Learning Methods
Abstract: We compare a variety of methods for predicting the probability of a binary treatment (the propensity score), with the goal of comparing otherwise like cases in treatment and control conditions for causal inference about treatment effects. Better prediction methods can under some circumstances improve causal inference both by reducing the finite sample bias and variability of estimators, but sometimes better predictions of the probability of treatment can increase bias and variance, and we clarify the conditions under which different methods produce better or worse inference (in terms of mean squared error of causal impact estimates). 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Nichols.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:13

Template-Type: ReDIF-Paper 1.0
Author-Name:  David Kantor
Author-Workplace-Name: Data for Decisions
Author-Email:  kantor.d@att.net
Author-Person: pka118
Title: Extended-value logic
Abstract: In 2001, I gave a presentation on three-valued logic. Since then, I have developed some ideas that grew out of that investigation, leading to new insights about missing values, and to the development of five-valued logic. I will also show how these notions extend to numeric computation and to an abstract generalization of the principles involved.
This is not about analysis; this is about data construction and preparation, and it is a possibly interesting conceptual tool.
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Kantor.pptx
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:14

Template-Type: ReDIF-Paper 1.0
Author-Name:  Geoff Dougherty
Author-Workplace-Name: Johns Hopkins Bloomberg School of Public Health
Author-Email:  geoffdougherty@gmail.com
Author-Name: Lorraine Dean
Author-Workplace-Name: Johns Hopkins Bloomberg School of Public Health
Title: Using theory to define a computationally tractable specification space in confirmatory factor modeling
Abstract: Researchers constructing measurement models must decide how to proceed when an initial specification fits poorly. Common approaches include search algorithms that optimize fit, and piecemeal changes to the item list or the error specification. The former approach may yield a good-fitting model that is inconsistent with theory, or may fail to identify the best-fitting model due to local optimization issues. The latter suffers from poor reproducibility and may also fail to identify the optimal model. We outline a new approach that defines a computationally tractable specification space based on theory.  We use the example of a hypothesized latent variable with 25 candidate indicators divided across five content areas. Using Stata’s  –tuples- command, we identify all combinations of indicators containing >=1 indicator per content area. In our example, this yields 7,294 models. We estimate each model on a derivation dataset and select candidate models with fit statistics that are acceptable, or could be rendered acceptable by permitting correlated errors. Eight models fit these criteria. We evaluate modification indices, re-specify if there is theoretical justification for correlated errors, and select a final model based on fit statistics. In contrast to other methods, this approach is easily replicable and may result in a model that is consistent with theory and has acceptable fit.
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Dougherty.pptx
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:16

Template-Type: ReDIF-Paper 1.0
Author-Name:  Joseph Terza
Author-Workplace-Name: Indiana University-Purdue University Indianapolis
Author-Email:  jvterza@iupui.edu
Author-Person: pte168
Author-Name: David M. Drukker
Author-Workplace-Name: StataCorp
Title: Stata Implementation of Alternative Residual Inclusion Estimators for Models with Endogenous Regressors
Abstract: Empirical analyses often require implementation of nonlinear models whose regressors include one or more endogenous variables – regressors that are correlated with the unobserved random component of the model.  Failure to account for such correlation in estimation leads to bias and produces results that are not causally interpretable.  Terza et al. (2008) discuss a relatively simple framework designed to explicitly account for such endogeneity – the residual inclusion (RI) framework.  They also give the analytic details of a corresponding two stage estimator that yields consistent parameter estimates in wide variety of nonlinear regression contexts – two-stage residual inclusion (2SRI).  The 2SRI estimates can be obtained using packaged Stata commands but the corresponding asymptotically correct standard errors (ACSE) require some analytic derivation and Mata coding [for details see Terza (2016)]. In the proposed presentation, we will discuss two alternative estimation approaches for RI models with a view toward broadening the menu of Stata implementation options for users who may not prefer to program in Mata or may be inclined to avoid analytic derivations:  generalized method of moments (GMM) [StataCorp (2015)]; and quasi limited information maximum likelihood (QLIML) [Wooldridge (2014)].  GMM can be applied using the packaged Stata gmm command and, although it requires that the user supply analytic formulae for the relevant moment conditions, it frees the user from the Mata coding required by 2SRI for calculation of the ACSE.  QLIML is implemented via the Mata optimize command so it does require some knowledge of Mata coding.  On the other hand, it does not place any analytic demands on the user for calculation of the parameter estimates or their ACSE.  We will detail all three of these approaches and, in the context of an empirical example, we will give template code for their Stata implementation (including calculation of the ACSE).  We note that, although the methods are essentially asymptotically equivalent the methods yield different results in the context of our example.  We offer analytic explanations for these differences. We also apply the methods to Monte Carlo simulated samples to further elucidate their implementation. Such simulations also serve to validate their large sample properties and reveal aspects of finite sample performance. Because the methods are essentially asymptotically equivalent, we conclude that one’s choice of approach should depend solely on the user’s coding preferences and his proclivity for analytic derivation.  It is hoped that this presentation will broaden Stata users’ access to this important class of models (the RI framework) for the specification and estimation of econometric models involving endogenous regressors.
Terza, J., Basu, A. and Rathouz, P. (2008):  “Two-Stage Residual Inclusion Estimation:  Addressing Endogeneity in Health Econometric Modeling,” Journal of Health Economics, 27, 531-543.
Terza, J.V. (2016):  “Simpler Standard Errors for Two-Stage Optimization Estimators,”  Stata Journal, 16, 368-385.
Wooldridge, J.M. (2014):  Quasi-Maximum Likelihood Estimation and Testing for Nonlinear Models with Endogenous Explanatory Variables,” Journal of Econometrics, 182, 226-234. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Drukker.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:17

Template-Type: ReDIF-Paper 1.0
Author-Name:  Jairo G Isaza Castro
Author-Workplace-Name: Universidad de la Salle
Author-Email:  jisaza@lasalle.edu.co
Author-Person: pis107
Author-Name: Karen Hernandez
Author-Workplace-Name: Universidad de la Salle
Author-Name: Karen Guerrero
Author-Workplace-Name: Universidad de la Salle
Author-Name: Jessy Hemer
Author-Workplace-Name: Universidad de la Salle
Title: Computing occupational segregation indices with standard errors: an ado file application with an illustration for Colombia
Abstract: We developed an ado file in order to estimate in an easy way three selected occupational segregation indicators with standard errors using a bootstrap procedure. The indicators are the Duncan and Duncan (1955) dissimilarity index, the Gini coefficient based on the distribution of jobs by gender (see Deutsch et al., 1994)) and the Karmel & MacLachlan (1988) index of labour market segregation. This routine can be easily applied to conventional labor market microdata in which information regarding the occupation classification, industry and occupational category variables are usually available. As an illustration of the application of this ado file, we present estimates of both occupational and industry segregation by gender drawn from household surveys' Colombian microdata. The estimation of occupational segregation measures with standard errors proves to be useful in assessing statistical differences in segregation measures within labor market groups and/or over time. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Isaza-Castro.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:18

Template-Type: ReDIF-Paper 1.0
Author-Name:  Kevin Krost
Author-Workplace-Name: Virginia Tech
Author-Email:  kevinkrost@vt.edu
Author-Name: Joshua Cohen 
Author-Workplace-Name: Virginia Tech
Title: Detection and Prediction of Gender-Based Differential Item Functioning using the MIMIC Model
Abstract: There has been extensive research indicating gender-based differences among STEM subjects, particularly mathematics (Albano & Rodriguez, 2013; Lane, Wang, & Magone, 1996). Similarly, gender-based differential item functioning (DIF) has been researched due to the disadvantages females face in STEM subjects when compared to their male counterparts. Given that, this study will apply the multiple indicators multiple causes (MIMIC) model, a type of structural equation model, to detect the presence of gender-based DIF using the Program for International Student Assessment (PISA) mathematics data from students in the United States of America then predict the DIF using math-related covariates. This study will build upon a previous study which explored the same data using the hierarchical generalized linear model and will be confirmatory in nature. Based on the results of the previous study, it is expected that several items will exhibit DIF which disadvantages females, and that mathematics-based self-efficacy will predict the DIF. However, additional covariates will also be explored and the two models will be compared in terms of their DIF-detection and the subsequent modeling of DIF. Implications of these results include females under-achieving when compared to their male counterparts, thus continuing the current trend. These gender differences can further manifest at the national level, causing US students as a whole to under-perform at the international level. Last, the efficacy of the MIMIC model to detect and predict DIF will be illustrated and become increasingly used to model and better understand differences and DIF. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Krost.pptx
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:19

Template-Type: ReDIF-Paper 1.0
Author-Name:  Minh Nguyen
Author-Workplace-Name: World Bank
Author-Email:  mnguyen3@worldbank.org
Author-Name: Paul Andres Corral Rodas
Author-Workplace-Name: World Bank
Author-Person: pco815
Author-Name: Joao Pedro Wagner De Azevedo
Author-Workplace-Name: World Bank
Author-Person: pwa88
Author-Name: Qinghua Zhao 
Author-Workplace-Name: World Bank
Title: Small area estimation/Poverty Map in Stata
Abstract: We present a new Stata package for small area estimations of poverty and inequality implementing methodologies from Elbers, Lanjouw, and Lanjouw (Econometrica, 2003). Small area methods attempt to solve low representativeness of surveys within areas, or the lack of data for specific areas/sub-populations. This is accomplished by incorporating information from outside sources. A common outside source is census data which often lacks detailed information on welfare. Thus far, a major limitation towards such analysis in Stata has been the memory required to work with census data . The povmap package introduces new mata functions and a plugin used to circumvent memory limitations that will arise when working with big data.  
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Nguyen.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:20

Template-Type: ReDIF-Paper 1.0
Author-Name:  Ali Lauer
Author-Workplace-Name: Abt Associates
Author-Email:  ali_lauer@abtassoc.com 
Title: Interactive maps
Abstract: We present examples of how to construct interactive maps in Stata, using only built-in commands available even in secure environments. One can also use built-in commands to smooth geographic data as a pre-processing step. Smoothing can be done using methods from twoway contour, or predictions from a GMM model as described in Drukker , Prucha, and Raciborski (2013; SJ 13-2 pp.287-301). The basic approach to creating a map in Stata is twoway area, with the options nodropbase cmiss(no) yscale(off) xscale(off), with a polygon “shape file” dataset (often created by the user-written shp2dta by Kevin Crow, possible with a change of projection using programs by Robert Picard), and multiple calls to area with if qualifiers to build a choropleth or scatter to superimpose point data. This approach is automated by several user-written commands, and works well for static images, but is less effective for web content where a Javascript entity is desirable. However, it is straightforward to write out the requisite information using the file command, and to use open-source map tools to create interactive maps for the web. We present two useful examples. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Lauer.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:21

Template-Type: ReDIF-Paper 1.0
Author-Name:  Hiren Nisar
Author-Workplace-Name: Abt Associates
Title: Analyzing satellite data in Stata
Abstract: We provide examples of how one can use satellite or other remote sensing data in Stata, with a variety of analysis methods, including examples of measuring economic disadvantage using satellite imagery. 
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Nisar.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:22

Template-Type: ReDIF-Paper 1.0
Author-Name:  Matthew P. Rabbitt
Author-Workplace-Name: Economic Research Service, U.S. Department of Agriculture
Author-Email:  matthew.rabbitt@ers.usda.gov
Author-Person: pra682
Title: Estimating Treatment Effects in the Presence of Correlated Binary Outcomes and Contemporaneous Selection
Abstract: Estimating the causal effect of a treatment is challenging when selection into the treatment is based on contemporaneous unobservable characteristics, and the outcome of interest is represented by a series of correlated binary outcomes. Under these assumptions, traditional non-linear panel data models, such as the random-effects logistic model, will produce biased estimates of the treatment effect due to correlation between the treatment variable and model unobservables. In this presentation, I will introduce a new Stata estimation command, ETXTLOGIT, which can estimate a model where the outcome is a series of J correlated logistic binary outcomes and selection into the treatment is based on contemporaneous unobservable characteristics. The presentation will introduce the new estimation command, present Monte Carlo evidence, and offer empirical examples. Special cases of the model will be discussed, including applications based on the explanatory (behavioral) Rasch model, a model from Item Response Theory (IRT).  
File-URL: http://fmwww.bc.edu/repec/scon2017/Baltimore17_Rabbitt.pdf
Creation-Date: 20170810 
Handle: RePEc:boc:scon17:23

